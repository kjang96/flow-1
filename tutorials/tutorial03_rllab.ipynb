{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 03: Running rllab Experiments\n",
    "\n",
    "This tutorial walks you through the process of running traffic simulations in Flow with trainable rllab-powered agents. Autonomous agents will learn to maximize a certain reward over the rollouts, using the **rllab** library [1]. Simulations of this form will depict the propensity of RL agents to influence the traffic of a human fleet in order to make the whole fleet more efficient (for some given metrics). \n",
    "\n",
    "In this exercise, we simulate an initially perturbed single lane ring road, where we introduce a single autonomous vehicle. We witness that, after some training, that the autonomous vehicle learns to dissipate the formation and propagation of \"phantom jams\" which form when only human driver dynamics is involved.\n",
    "\n",
    "## 1. Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *scenario*, and an *environment*. Scenarios describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc... in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act as the primary interface between the reinforcement learning algorithm and the scenario. Moreover, custom environments may be used to modify the dynamical features of an scenario. Finally, in the RL case, it is in the *environment* that the state/action spaces and the reward function are defined. \n",
    "\n",
    "## 2. Setting up a Scenario\n",
    "Flow contains a plethora of pre-designed scenarios used to replicate highways, intersections, and merges in both closed and open settings. All these scenarios are located in flow/scenarios. For this exercise, which involves a single lane ring road, we will use the scenario `LoopScenario`.\n",
    "\n",
    "### 2.1 Setting up Scenario Parameters\n",
    "\n",
    "The scenario mentioned at the start of this section, as well as all other scenarios in Flow, are parameterized by the following arguments: \n",
    "* name\n",
    "* generator_class\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "* traffic_lights\n",
    "\n",
    "These parameters are explained in detail in exercise 1. Moreover, all parameters excluding vehicles (covered in section 2.2) do not change from the previous exercise. Accordingly, we specify them as we have before, and leave further explanations of the parameters to exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ring road scenario class\n",
    "from flow.scenarios.loop.loop_scenario import LoopScenario\n",
    "\n",
    "# ring road generator class\n",
    "from flow.scenarios.loop.gen import CircleGenerator\n",
    "\n",
    "# input parameter classes to the scenario class\n",
    "from flow.core.params import NetParams, InitialConfig\n",
    "\n",
    "# name of the scenario\n",
    "name = \"training_example\"\n",
    "\n",
    "# network-specific parameters\n",
    "from flow.scenarios.loop.loop_scenario import ADDITIONAL_NET_PARAMS\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)\n",
    "\n",
    "# initial configuration to vehicles\n",
    "initial_config = InitialConfig(spacing=\"uniform\", perturbation=1)\n",
    "\n",
    "# traffic lights (empty)\n",
    "from flow.core.traffic_lights import TrafficLights\n",
    "traffic_lights = TrafficLights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Adding Trainable Autonomous Vehicles\n",
    "The `Vehicles` class stores state information on all vehicles in the network. This class is used to identify the dynamical features of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various `get` methods within this class.\n",
    "\n",
    "The dynamics of vehicles in the `Vehicles` class can either be depicted by sumo or by the dynamical methods located in flow/controllers. For human-driven vehicles, we use the IDM model for acceleration behavior, with exogenous gaussian acceleration noise with std 0.2 m/s2 to induce perturbations that produce stop-and-go behavior. In addition, we use the `ContinousRouter` routing controller so that the vehicles may maintain their routes closed networks.\n",
    "\n",
    "As we have done in exercise 1, human-driven vehicles are defined in the `Vehicles` class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicles class\n",
    "from flow.core.vehicles import Vehicles\n",
    "\n",
    "# vehicles dynamics models\n",
    "from flow.controllers import IDMController, ContinuousRouter\n",
    "\n",
    "vehicles = Vehicles()\n",
    "vehicles.add(\"human\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above addition to the `Vehicles` class only accounts for 21 of the 22 vehicles that are placed in the network. We now add an additional trainable autuonomous vehicle whose actions are dictated by an RL agent. This is done by specifying an `RLController` as the acceleraton controller to the vehicle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.controllers import RLController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this controller serves primarirly as a placeholder that marks the vehicle as a component of the RL agent, meaning that lane changing and routing actions can also be specified by the RL agent to this vehicle.\n",
    "\n",
    "We finally add the vehicle as follows, while again using the `ContinuousRouter` to perpetually maintain the vehicle within the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles.add(veh_id=\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             routing_controller=(ContinuousRouter, {}),\n",
    "             num_vehicles=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Scenario Object\n",
    "\n",
    "We are finally ready to create the scenario object, as we had done in exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = LoopScenario(name=\"ring_example\",\n",
    "                        generator_class=CircleGenerator,\n",
    "                        vehicles=vehicles,\n",
    "                        net_params=net_params,\n",
    "                        initial_config=initial_config,\n",
    "                        traffic_lights=traffic_lights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several environments in Flow exist to train RL agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. The use of an environment allows us to view the cumulative reward simulation rollouts receive, along with to specify the state/action spaces.\n",
    "\n",
    "Envrionments in Flow are parametrized by three components:\n",
    "* env_params\n",
    "* sumo_params\n",
    "* scenario\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length of any simulation step and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and activate the GUI. \n",
    "\n",
    "**Note** For training purposes, it is highly recommanded to deactivate the GUI in order to avoid global slow down. In such case, one just need to specify the following: `render=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams(sim_step=0.1, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specifies environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the scenario. For the environment \"WaveAttenuationPOEnv\", these parameters are used to dictate bounds on the accelerations of the autonomous vehicles, as well as the range of ring lengths (and accordingly network densities) the agent is trained on.\n",
    "\n",
    "Finally, it is important to specify here the *horizon* of the experiment, which is the duration of one episode (during which the RL-agent acquire data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "env_params = EnvParams(\n",
    "    # length of one rollout\n",
    "    horizon=100,\n",
    "\n",
    "    additional_params={\n",
    "        # maximum acceleration of autonomous vehicles\n",
    "        \"max_accel\": 1,\n",
    "        # maximum deceleration of autonomous vehicles\n",
    "        \"max_decel\": 1,\n",
    "        # bounds on the ranges of ring road lengths the autonomous vehicle \n",
    "        # is trained on\n",
    "        \"ring_length\": [220, 270],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Initializing a Gym Environments\n",
    "\n",
    "Now, we have to specify our Gym Environment and the algorithm that our RL agents we'll use. To specify the environment, one has to use the environment's name (a simple string). A list of all environment names is located in `flow/envs/__init__.py`. The names of available environments can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flow.envs as flowenvs\n",
    "\n",
    "print(flowenvs.__all__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the environment \"WaveAttenuationPOEnv\", which is used to train autonomous vehicles to attenuate the formation and propagation of waves in a partially observable variable density ring road. To create the Gym Environment, the only necessary parameters are the environment name plus the previously defined variables. These are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_name = \"WaveAttenuationPOEnv\"\n",
    "pass_params = (env_name, sumo_params, vehicles, env_params, net_params,\n",
    "               initial_config, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up and Running an RL Experiment\n",
    "\n",
    "### 4.1 run_task\n",
    "We begin by creating a `run_task` method, which defines various components of the RL algorithm within rllab, such as the environment, the type of policy, the policy training method, etc.\n",
    "\n",
    "We create the gym environment defined in section 3 using the `GymEnv` function.\n",
    "\n",
    "In this experiment, we use a Gaussian MLP policy: we just need to specify its dimensions `(32,32)` and the environment name. We'll use linear baselines and the Trust Region Policy Optimization (TRPO) algorithm (see https://arxiv.org/abs/1502.05477):\n",
    "- The `batch_size` parameter specifies the size of the batch during one step of the gradient descent. \n",
    "- The `max_path_length` parameter indicates the biggest rollout size possible of the experiment. \n",
    "- The `n_itr` parameter gives the number of iterations used in training the agent.\n",
    "\n",
    "In the following, we regroup all the previous commands in one single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllab.algos.trpo import TRPO\n",
    "from rllab.baselines.linear_feature_baseline import LinearFeatureBaseline\n",
    "from rllab.policies.gaussian_mlp_policy import GaussianMLPPolicy\n",
    "from rllab.envs.normalized_env import normalize\n",
    "from rllab.envs.gym_env import GymEnv\n",
    "\n",
    "def run_task(*_):\n",
    "    env = GymEnv(env_name, record_video=False, register_params=pass_params)\n",
    "    horizon = env.horizon\n",
    "    env = normalize(env)\n",
    "\n",
    "    policy = GaussianMLPPolicy(\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(32, 32)\n",
    "    )\n",
    "\n",
    "    baseline = LinearFeatureBaseline(env_spec=env.spec)\n",
    "\n",
    "    algo = TRPO(\n",
    "        env=env,\n",
    "        policy=policy,\n",
    "        baseline=baseline,\n",
    "        batch_size=1000,\n",
    "        max_path_length=horizon,\n",
    "        discount=0.999,\n",
    "        n_itr=1,\n",
    "    )\n",
    "    algo.train(),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 run_experiment_lite\n",
    "\n",
    "Using the above `run_task` method, we will execute the training process using rllab's `run_experiment_lite` methods. In this method, we are able to specify:\n",
    "- The `n_parallel` cores you want to use for your experiment. If you set `n_parallel`>1, two processors will execute your code in parallel which results in a global roughly linear speed-up.\n",
    "- The `snapshot_mode`, which specifies how frequently (blank).\n",
    "- The `mode` which can set to be *local* is you want to run the experiment locally, or to *ec2* for launching the experiment on an Amazon Web Services instance.\n",
    "- The `seed` parameter which calibrates the randomness in the experiment. \n",
    "- The `tag`, or name, for your experiment.\n",
    "\n",
    "Finally, we are ready to begin the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rllab.misc.instrument import run_experiment_lite\n",
    "\n",
    "for seed in [5]:  # , 20, 68]:\n",
    "    run_experiment_lite(\n",
    "        run_task,\n",
    "        # Number of parallel workers for sampling\n",
    "        n_parallel=1,\n",
    "        # Keeps the snapshot parameters for all iterations\n",
    "        snapshot_mode=\"all\",\n",
    "        # Specifies the seed for the experiment. If this is not provided, a\n",
    "        # random seed will be used\n",
    "        seed=seed,\n",
    "        mode=\"local\",\n",
    "        exp_prefix=\"training_example\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Bibliography\n",
    "[1] Yan Duan, Xi Chen, Rein Houthooft, John Schulman, Pieter Abbeel. \"Benchmarking Deep Reinforcement Learning for Continuous Control\". Proceedings of the 33rd International Conference on Machine Learning (ICML), 2016.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
